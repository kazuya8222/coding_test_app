import React, { useState, useEffect, useRef } from 'react';
import { useParams, useNavigate } from 'react-router-dom';
import axios from 'axios';
import { InterviewProblem } from '../../../../shared/types/interview';
import { Modal } from '../Modal';
import { axiosInstance } from '../../api/axios';
const API_URL = import.meta.env.VITE_API_URL;

// Configuration for OpenAI Whisper and GPT-4o
const OPENAI_API_URL = import.meta.env.OPENAI_API_URL || 'https://api.openai.com/v1';

interface Message {
  role: 'user' | 'assistant';
  content: string;
}

interface InterviewQuestion {
  id: string;
  question: string;
  asked: boolean;
}

export const VideoInterviewScreen: React.FC = () => {
  // URL params and navigation
  const { problemId } = useParams<{ problemId: string }>();
  const navigate = useNavigate();
  
  // State for media handling
  const videoRef = useRef<HTMLVideoElement>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const [stream, setStream] = useState<MediaStream | null>(null);
  const [recording, setRecording] = useState(false);
  const [audioChunks, setAudioChunks] = useState<Blob[]>([]);
  const [transcription, setTranscription] = useState('');
  
  // Audio visualization
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationRef = useRef<number | null>(null);
  const [isAnimating, setIsAnimating] = useState(false);
  
  // Interview state
  const [problem, setProblem] = useState<InterviewProblem | null>(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  const [conversationHistory, setConversationHistory] = useState<Message[]>([]);
  const [currentQuestionIndex, setCurrentQuestionIndex] = useState(0);
  const [questions, setQuestions] = useState<InterviewQuestion[]>([]);
  const [interviewStarted, setInterviewStarted] = useState(false);
  const [interviewEnded, setInterviewEnded] = useState(false);
  const [interviewSessionId, setInterviewSessionId] = useState<string | null>(null);
  const [loadingResponse, setLoadingResponse] = useState(false);
  
  // Audio playback state
  const [isAITalking, setIsAITalking] = useState(false);
  const [currentAudioMessage, setCurrentAudioMessage] = useState<string>('');
  
  // Countdown state
  const [countdown, setCountdown] = useState<number | null>(null);
  const [showModal, setShowModal] = useState(false);
  
  // Interview metrics
  const [startTime, setStartTime] = useState<Date | null>(null);
  const [duration, setDuration] = useState(0);
  const [feedbackText, setFeedbackText] = useState('');

  // Load problem details and initialize questions
  useEffect(() => {
    const fetchProblem = async () => {
      try {
        const response = await axiosInstance.get(`${API_URL}/video-interviews/${problemId}`);
        setProblem(response.data);
        
        let parsedQuestions: InterviewQuestion[] = [];
        
        // Parse questions from question_script
        if (response.data.question_script) {
          // Split by newlines and filter out empty lines
          const lines = response.data.question_script.split('\n')
            .map(line => line.trim())
            .filter(line => line.length > 0);
          
          // Process each line to extract questions
          lines.forEach((line, index) => {
            // Check if line contains a question mark or looks like a question
            if (line.includes('?') || line.toLowerCase().startsWith('how') || 
                line.toLowerCase().startsWith('what') || line.toLowerCase().startsWith('why')) {
              parsedQuestions.push({
                id: `q-${index}`,
                question: line,
                asked: false
              });
            }
          });
        }
        
        // Add follow-up questions if available
        if (response.data.follow_up_questions && Array.isArray(response.data.follow_up_questions)) {
          response.data.follow_up_questions.forEach((q: string, i: number) => {
            if (q && q.trim()) {
              parsedQuestions.push({
                id: `fq-${i}`,
                question: q.trim(),
                asked: false
              });
            }
          });
        }
        
        // If we still don't have enough questions, add generic ones
        if (parsedQuestions.length < 3) {
          const genericQuestions = [
            "Can you tell me about your experience with this technology?",
            "How would you approach this problem in a production environment?",
            "What challenges do you foresee with this solution?"
          ];
          
          while (parsedQuestions.length < 3) {
            parsedQuestions.push({
              id: `gq-${parsedQuestions.length}`,
              question: genericQuestions[parsedQuestions.length % genericQuestions.length],
              asked: false
            });
          }
        }
        
        // Set the questions state
        setQuestions(parsedQuestions);
        setLoading(false);
      } catch (error) {
        console.error('Error fetching problem:', error);
        setError('Failed to load problem');
        setLoading(false);
      }
    };

    fetchProblem();
  }, [problemId]);

  // Request camera and microphone access
  // MediaStreamå–å¾—ã¨å‡¦ç†ã®ä¿®æ­£

// Request camera and microphone access
useEffect(() => {
  const requestMediaPermission = async () => {
    try {
      // ã¾ãšæ˜ åƒã®ã¿ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å–å¾—ï¼ˆéŸ³å£°ãªã—ï¼‰
      const videoOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: true,
        audio: false
      });
      
      // éŸ³å£°ã®ã¿ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’åˆ¥é€”å–å¾—
      const audioOnlyStream = await navigator.mediaDevices.getUserMedia({
        video: false,
        audio: true
      });
      
      // ãƒ“ãƒ‡ã‚ªè¦ç´ ã«ã¯æ˜ åƒã®ã¿ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‰²ã‚Šå½“ã¦
      if (videoRef.current) {
        videoRef.current.srcObject = videoOnlyStream;
        // å¿µã®ãŸã‚ muted ã‚’ç¢ºå®Ÿã«è¨­å®š
        videoRef.current.muted = true;
      }
      
      // ä¸¡æ–¹ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä¿å­˜ï¼ˆéŒ²ç”»ç”¨ã«å¿…è¦ãªå ´åˆï¼‰
      // æ³¨: å®Ÿéš›ã®éŒ²ç”»æ™‚ã«ã¯ä¸¡æ–¹ã®ãƒˆãƒ©ãƒƒã‚¯ã‚’çµ„ã¿åˆã‚ã›ã‚‹å‡¦ç†ãŒå¿…è¦
      setStream(audioOnlyStream);
      
      // éŸ³å£°åˆ†æç”¨ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
      const audioContext = new AudioContext();
      audioContextRef.current = audioContext;
      
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      analyserRef.current = analyser;
      
      const source = audioContext.createMediaStreamSource(audioOnlyStream);
      source.connect(analyser);
      // ã“ã“ã§ destination ã«ã¯æ¥ç¶šã—ãªã„
      // ä¸‹è¨˜ã®è¡Œã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ãŠã
      // source.connect(audioContext.destination);
      
      // éŸ³å£°ã®å¯è¦–åŒ–é–‹å§‹
      setIsAnimating(true);
    } catch (err) {
      console.error('Error accessing media devices:', err);
      setError('Failed to access camera or microphone. Please ensure permissions are granted.');
    }
  };
  
  requestMediaPermission();
  
  // Cleanup function to stop all tracks when component unmounts
  return () => {
    if (stream) {
      stream.getTracks().forEach(track => track.stop());
    }
    
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
    }
    
    if (audioContextRef.current) {
      audioContextRef.current.close();
    }
  };
}, []);


  // Audio visualization when the interview is active
  useEffect(() => {
    if (!canvasRef.current || !analyserRef.current || !isAnimating) return;
    
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');
    if (!ctx) return;
    
    const analyser = analyserRef.current;
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    const WIDTH = canvas.width;
    const HEIGHT = canvas.height;
    
    const renderFrame = () => {
      animationRef.current = requestAnimationFrame(renderFrame);
      
      analyser.getByteFrequencyData(dataArray);
      
      ctx.clearRect(0, 0, WIDTH, HEIGHT);
      
      // Background
      ctx.fillStyle = 'rgba(0, 0, 0, 0.1)';
      ctx.fillRect(0, 0, WIDTH, HEIGHT);
      
      const barWidth = (WIDTH / bufferLength) * 2.5;
      let x = 0;
      
      // Determine who is speaking and use appropriate color
      const color = isAITalking ? '#6366F1' : '#2563EB'; // Indigo for AI, Blue for user
      
      for (let i = 0; i < bufferLength; i++) {
        const barHeight = dataArray[i] / 2; // Scale down to fit better
        
        ctx.fillStyle = color;
        ctx.fillRect(x, HEIGHT - barHeight, barWidth, barHeight);
        
        x += barWidth + 1;
      }
    };
    
    renderFrame();
    
    return () => {
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current);
      }
    };
  }, [isAnimating, isAITalking]);

  // Automatically start the interview
  useEffect(() => {
    // If we have the problem and we're already on this screen, start the interview
    if (problem && !interviewStarted && !loading) {
      startInterview();
    }
  }, [problem, loading]);

  // Start interview session
  const startInterview = async () => {
    if (!problemId) return;
    
    try {
      // Create a new interview session
      const response = await axiosInstance.post(`${API_URL}/video-interviews/${problemId}/start`);
      console.log(response.data);
      setInterviewSessionId(response.data._id);
      
      // Set start time
      const now = new Date();
      setStartTime(now);
      
      // Add initial message from AI interviewer
      const initialMessage = {
        role: 'assistant' as const,
        content: `ã“ã‚“ã«ã¡ã¯ï¼æœ¬æ—¥ã®é¢æ¥å®˜ã‚’æ‹…å½“ã—ã¾ã™ã€‚ã€${problem?.title}ã€ã«ã¤ã„ã¦è©±ã—åˆã£ã¦ã„ãã¾ã—ã‚‡ã†ã€‚å…¨ã¦ã§3ã¤ã®è³ªå•ã‚’ã—ã¾ã™ã€‚ã¾ãšã¯æœ€åˆã®è³ªå•ã‹ã‚‰ã¯ã˜ã‚ã¾ã—ã‚‡ã†ï¼š${questions[0].question}`
      };
      
      setCurrentAudioMessage(initialMessage.content);
      setIsAITalking(true);
      setIsAnimating(true);
      
      await speakTextWithOpenAI(initialMessage.content, () => {
        setIsAITalking(false);
        startRecording();
      });
      
      setConversationHistory([initialMessage]);
      
      // Mark first question as asked
      const updatedQuestions = [...questions];
      updatedQuestions[0].asked = true;
      setQuestions(updatedQuestions);
      
      setInterviewStarted(true);
    } catch (error) {
      console.error('Error starting interview:', error);
      setError('Failed to start interview session');
    }
  };

  // Handle starting the recording
  const startRecording = () => {
    if (!stream) return;
  
    const mediaRecorder = new MediaRecorder(stream);
    mediaRecorderRef.current = mediaRecorder;
  
    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        setAudioChunks(prev => [...prev, event.data]);
      }
    };
  
    mediaRecorder.onstop = handleRecordingStopped;
  
    setAudioChunks([]);
    mediaRecorder.start();
    setRecording(true);
  
    // ğŸ¯ ç„¡éŸ³æ¤œå‡ºç”¨: ç›£è¦–ã‚’é–‹å§‹
    monitorSilence();
  };
  

  // Process audio after recording stops
  const handleRecordingStopped = async () => {
    if (audioChunks.length === 0) return;
    
    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
    
    // Create a FormData object to send the audio
    const formData = new FormData();
    formData.append('file', audioBlob, 'recording.webm');
    formData.append('model', 'whisper-1');
    
    try {
      setLoadingResponse(true);
      
      // Send to Whisper API for transcription
      const response = await axios.post(
        `${OPENAI_API_URL}/audio/transcriptions`, 
        formData,
        {
          headers: {
            'Authorization': `Bearer ${import.meta.env.VITE_OPENAI_API_KEY}`,
            'Content-Type': 'multipart/form-data'
          }
        }
      );
      
      const transcribedText = response.data.text;
      setTranscription(transcribedText);
      
      // Add user message to conversation
      const userMessage = {
        role: 'user' as const,
        content: transcribedText
      };
      
      setConversationHistory(prev => [...prev, userMessage]);
      
      // Get AI response
      await getFollowUpQuestion(transcribedText);
      
      setLoadingResponse(false);
    } catch (error) {
      console.error('Error processing audio:', error);
      setLoadingResponse(false);
      setError('Failed to process recording');
    }
  };

  // Get response from GPT-4o
  const getAIResponse = async (userMessage: string) => {
    if (!problemId || !interviewSessionId) return;
    
    try {
      // Determine next action based on current question index
      const nextIndex = currentQuestionIndex + 1;
      
      // Check if this was the last question
      const isLastQuestion = nextIndex >= questions.length;
      
      let aiResponse;
      
      if (isLastQuestion) {
        // If all questions have been asked, end the interview
        aiResponse = "Thank you for your thoughtful responses. We've now completed all the questions for this interview. I appreciate your time and insights. Is there anything else you'd like to add or any questions you have for me before we conclude?";
        setInterviewEnded(true);
      } else {
        // Ask the next question
        const nextQuestion = questions[nextIndex].question;
        aiResponse = `Thank you for your response. Let's move on to the next question: ${nextQuestion}`;
        
        // Mark question as asked
        const updatedQuestions = [...questions];
        updatedQuestions[nextIndex].asked = true;
        setQuestions(updatedQuestions);
        
        // Update current question index
        setCurrentQuestionIndex(nextIndex);
      }
      
      // Add AI response to conversation
      const assistantMessage = {
        role: 'assistant' as const,
        content: aiResponse
      };
      
      setConversationHistory(prev => [...prev, assistantMessage]);
      
      // Save conversation to the interview session
      await axiosInstance.post(`${API_URL}/video-interviews/${interviewSessionId}/message`, {
        messages: [
          { speaker: 'user', message: userMessage },
          { speaker: 'ai', message: aiResponse }
        ]
      });
      
      // Set current audio message and indicate AI is talking
      setCurrentAudioMessage(aiResponse);
      setIsAITalking(true);
      
      // Start TTS
      await speakTextWithOpenAI(aiResponse, () => {
        setIsAITalking(false);
        if (!isLastQuestion) {
          startRecording();
        }
      });
    } catch (error) {
      console.error('Error getting AI response:', error);
      setError('Failed to get interviewer response');
    }
  };
  const getFollowUpQuestion = async (userAnswer: string) => {
    try {
      const response = await axios.post(`${API_URL}/followup`, {
        userAnswer,
      });
  
      const followUp = response.data.followUpQuestion;
  
      setCurrentAudioMessage(followUp);
      setIsAITalking(true);
  
      await speakTextWithOpenAI(followUp, () => {
        setIsAITalking(false);
        startRecording();
      });
    } catch (error) {
      console.error('Error fetching follow-up:', error);
      setError('Failed to fetch follow-up question');
    }
  };
  

  // Speech synthesis with OpenAI TTS
  let currentAudio: HTMLAudioElement | null = null;

  const speakTextWithOpenAI = async (text: string, onEndCallback?: () => void) => {
    try {
      // å‰ã®éŸ³å£°ãŒå†ç”Ÿä¸­ãªã‚‰å¿…ãšåœæ­¢ã™ã‚‹
      if (currentAudio) {
        currentAudio.pause();
        URL.revokeObjectURL(currentAudio.src); // ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é˜²ã
        currentAudio = null;
      }
  
      const response = await fetch(`${API_URL}/tts`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: text }),
      });
  
      if (!response.ok) {
        throw new Error('Failed to generate speech');
      }
  
      const audioBlob = await response.blob();
      const audioUrl = URL.createObjectURL(audioBlob);
  
      // æ–°ã—ã„ Audio ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
      const audio = new Audio(audioUrl);
      currentAudio = audio;
      
      // å†ç”ŸãŒçµ‚äº†ã—ãŸã¨ãã®å‡¦ç†
      audio.onended = () => {
        URL.revokeObjectURL(audioUrl); // ä¸è¦ã«ãªã£ãŸURLã‚’è§£æ”¾
        currentAudio = null;
        if (onEndCallback) {
          onEndCallback();
        }
      };
      
      // å†ç”Ÿé–‹å§‹
      await audio.play();
    } catch (error) {
      console.error('Error generating or playing TTS:', error);
      if (onEndCallback) {
        onEndCallback();
      }
    }
  };

  const monitorSilence = () => {
    if (!analyserRef.current) return;
  
    const analyser = analyserRef.current;
    const bufferLength = analyser.fftSize;
    const dataArray = new Uint8Array(bufferLength);
  
    let silenceStart = Date.now();
    const silenceThreshold = 15; // 0ã€œ255ã®ã‚¹ã‚±ãƒ¼ãƒ«ã€ã“ã“ã§ã¯å°ã•ã„å€¤ã§ç„¡éŸ³ã¨åˆ¤å®š
    const maxSilenceDuration = 3000; // 3ç§’
  
    const checkSilence = () => {
      if (!recording) return; // éŒ²éŸ³ã—ã¦ãªã‹ã£ãŸã‚‰ç›£è¦–çµ‚äº†
  
      analyser.getByteTimeDomainData(dataArray);
  
      // éŸ³é‡ï¼ˆæŒ¯å¹…ï¼‰ã®æœ€å¤§å€¤ã‚’è¨ˆç®—
      let maxVolume = 0;
      for (let i = 0; i < bufferLength; i++) {
        const v = Math.abs(dataArray[i] - 128); // 128ä¸­å¿ƒ
        if (v > maxVolume) {
          maxVolume = v;
        }
      }
  
      if (maxVolume < silenceThreshold) {
        // ç„¡éŸ³ãŒç¶šã„ã¦ã„ã‚‹
        if (Date.now() - silenceStart > maxSilenceDuration) {
          console.log('ç„¡éŸ³æ¤œå‡º â†’ è‡ªå‹•åœæ­¢');
          stopRecording();
          return; // åœæ­¢ã—ãŸã®ã§ç›£è¦–ã‚‚çµ‚äº†
        }
      } else {
        // éŸ³ã‚’æ¤œçŸ¥ â†’ ç„¡éŸ³ã‚¿ã‚¤ãƒãƒ¼ãƒªã‚»ãƒƒãƒˆ
        silenceStart = Date.now();
      }
  
      requestAnimationFrame(checkSilence); // ç¶™ç¶šç›£è¦–
    };
  
    requestAnimationFrame(checkSilence);
  };
  const stopRecording = () => {
    if (mediaRecorderRef.current && recording) {
      console.log('stopRecording called');
      mediaRecorderRef.current.stop();
      // recordingãƒ•ãƒ©ã‚°ã¯stopæ™‚ã«handleRecordingStoppedå´ã§ç®¡ç†ã™ã‚‹
    }
  };

  // Handle ending the interview
  const endInterview = async () => {
    if (!interviewSessionId) return;
    
    try {
      // Calculate duration
      if (startTime) {
        const endTime = new Date();
        const durationMs = endTime.getTime() - startTime.getTime();
        const durationMinutes = Math.round(durationMs / 60000);
        setDuration(durationMinutes);
      }
      
      // Submit interview completion
      const response = await axiosInstance.post(`${API_URL}/video-interviews/${interviewSessionId}/complete`, {
        transcript: conversationHistory.filter(msg => msg.role === 'user').map(msg => msg.content).join('\n\n'),
        question_responses: questions
          .filter(q => q.asked)
          .map((q, index) => {
            const userMsg = conversationHistory.filter(msg => msg.role === 'user')[index];
            return {
              question: q.question,
              response_transcript: userMsg ? userMsg.content : '',
              response_time: 60 // Default value, calculate actual time if needed
            };
          })
      });
    
      // Set feedback if available
      if (response.data.feedback) {
        setFeedbackText(response.data.feedback);
      } else {
        setFeedbackText('Thank you for completing the interview. Your responses have been recorded.');
      }
      
      // Navigate to results page
      navigate(`/interview/${interviewSessionId}/results`);
    } catch (error) {
      console.error('Error ending interview:', error);
      setError('Failed to submit interview results');
    }
  };

  if (loading) {
    return (
      <div className="flex items-center justify-center h-screen">
        <div className="animate-spin rounded-full h-12 w-12 border-t-2 border-b-2 border-primary"></div>
      </div>
    );
  }

  if (error || !problem) {
    return (
      <div className="flex items-center justify-center h-screen text-red-500">
        {error || 'Problem not found'}
      </div>
    );
  }

  return (
    <div className="h-screen bg-gray-100 flex flex-col">
      {/* Header */}
      <header className="bg-white shadow-sm py-4 px-6">
        <div className="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 flex justify-between items-center">
          <h1 className="text-2xl font-semibold text-gray-900">{problem.title}</h1>
          <div className="flex items-center space-x-4">
            {recording && (
              <span className="flex items-center text-red-600">
                <span className="w-3 h-3 bg-red-600 rounded-full mr-2 animate-pulse"></span>
                éŒ²éŸ³ä¸­
              </span>
            )}
            {interviewStarted && !interviewEnded && (
              <button
                onClick={endInterview}
                className="px-4 py-2 bg-red-600 text-white rounded-md hover:bg-red-700"
              >
                é¢æ¥ã‚’çµ‚äº†
              </button>
            )}
          </div>
        </div>
      </header>

      {/* Main content - Modern voice chat UI */}
      <main className="flex-grow flex flex-col overflow-hidden">
        <div className="flex-grow flex flex-col md:flex-row p-6 gap-6">
          {/* Left panel - Video and info */}
          <div className="w-full md:w-1/3 flex flex-col gap-4">
            {/* Video preview */}
            <div className="bg-gray-900 rounded-lg overflow-hidden aspect-video flex items-center justify-center">
              {stream ? (
                <video
                  ref={videoRef}
                  autoPlay
                  muted
                  className="w-full h-full object-cover"
                />
              ) : (
                <div className="text-gray-400">ã‚«ãƒ¡ãƒ©ã‚’èª­ã¿è¾¼ã¿ä¸­...</div>
              )}
            </div>
            
            {/* Interview info */}
            <div className="bg-white rounded-lg shadow p-4">
              <h3 className="text-lg font-medium text-gray-800 mb-3">é¢æ¥æƒ…å ±</h3>
              <div className="space-y-2 text-sm">
                <p><span className="font-medium">å•é¡Œ:</span> {problem.title}</p>
                <p><span className="font-medium">é›£æ˜“åº¦:</span> {problem.difficulty}</p>
                <p><span className="font-medium">ã‚¿ã‚¤ãƒ—:</span> {problem.interview_type}</p>
                {startTime && (
                  <p><span className="font-medium">çµŒéæ™‚é–“:</span> {Math.floor((Date.now() - startTime.getTime()) / 60000)} åˆ†</p>
                )}
              </div>
            </div>
            
            {/* Question progress */}
            <div className="bg-white rounded-lg shadow p-4">
              <h3 className="text-lg font-medium text-gray-800 mb-3">è³ªå•ã®é€²æ—</h3>
              <div className="space-y-3">
                {questions.map((q, index) => (
                  <div key={q.id} className="flex items-center">
                    <div className={`w-6 h-6 rounded-full flex items-center justify-center mr-3 ${
                      currentQuestionIndex > index 
                        ? 'bg-green-100 text-green-600' 
                        : currentQuestionIndex === index 
                          ? 'bg-blue-100 text-blue-600'
                          : 'bg-gray-100 text-gray-400'
                    }`}>
                      {currentQuestionIndex > index ? 'âœ“' : index + 1}
                    </div>
                    <p className={`text-sm ${
                      currentQuestionIndex > index 
                        ? 'text-gray-500 line-through' 
                        : currentQuestionIndex === index 
                          ? 'text-gray-900 font-medium'
                          : 'text-gray-400'
                    }`}>
                      {q.question.length > 50 ? q.question.substring(0, 50) + '...' : q.question}
                    </p>
                  </div>
                ))}
              </div>
            </div>
          </div>
          
          {/* Right panel - Voice chat UI */}
          <div className="w-full md:w-2/3 bg-white rounded-lg shadow flex flex-col">
            {/* Voice chat header */}
            <div className="p-4 border-b border-gray-200 flex items-center">
              <div className="w-10 h-10 rounded-full bg-indigo-100 flex items-center justify-center mr-3">
                <svg className="w-5 h-5 text-indigo-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M9.663 17h4.673M12 3v1m6.364 1.636l-.707.707M21 12h-1M4 12H3m3.343-5.657l-.707-.707m2.828 9.9a5 5 0 117.072 0l-.548.547A3.374 3.374 0 0014 18.469V19a2 2 0 11-4 0v-.531c0-.895-.356-1.754-.988-2.386l-.548-.547z"></path>
                </svg>
              </div>
              <h2 className="text-lg font-semibold text-gray-800">AIé¢æ¥å®˜</h2>
              {(recording || isAITalking) && (
                <span className={`ml-4 px-3 py-1 rounded-full text-xs font-medium ${
                  isAITalking 
                    ? 'bg-indigo-100 text-indigo-800' 
                    : 'bg-red-100 text-red-800'
                }`}>
                  {isAITalking ? 'è©±ã—ä¸­...' : 'èã„ã¦ã„ã¾ã™...'}
                </span>
              )}
            </div>
            
            {/* Voice visualization area */}
            <div className="flex-grow p-6 flex flex-col items-center justify-center">
              {interviewStarted ? (
                <>
                  {/* Current speaking text */}
                  <div className="w-full max-w-xl mb-8 text-center">
                    <p className="text-xl text-gray-700 font-medium">
                      {isAITalking ? currentAudioMessage : recording ? 'éŸ³å£°ã‚’èªè­˜ã—ã¦ã„ã¾ã™...' : ''}
                    </p>
                  </div>
                  
                  {/* Voice visualization canvas */}
                  <div className={`w-64 h-64 rounded-full flex items-center justify-center bg-gray-50 ${
                    isAITalking || recording ? 'border-4 border-blue-400 animate-pulse' : 'border border-gray-200'
                  }`}>
                    <canvas 
                      ref={canvasRef} 
                      width="256" 
                      height="256" 
                      className="rounded-full"
                    />
                    
                    {/* Dynamic circle in the middle */}
                    <div className={`absolute w-40 h-40 rounded-full bg-white flex items-center justify-center shadow-md 
                      ${isAITalking || recording ? 'animate-ping opacity-30' : 'opacity-0'}`}>
                    </div>
                    
                    <div className="absolute w-32 h-32 rounded-full bg-white flex items-center justify-center shadow-md">
                      {isAITalking ? (
                        <svg className="w-16 h-16 text-indigo-500" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                          <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M9.663 17h4.673M12 3v1m6.364 1.636l-.707.707M21 12h-1M4 12H3m3.343-5.657l-.707-.707m2.828 9.9a5 5 0 117.072 0l-.548.547A3.374 3.374 0 0014 18.469V19a2 2 0 11-4 0v-.531c0-.895-.356-1.754-.988-2.386l-.548-.547z"></path>
                        </svg>
                      ) : recording ? (
                        <svg className="w-16 h-16 text-red-500" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                          <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"></path>
                        </svg>
                      ) : (
                        <svg className="w-16 h-16 text-gray-300" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                          <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"></path>
                        </svg>
                      )}
                    </div>
                  </div>
                  
                  {/* Status text */}
                  <div className="mt-8 text-center">
                    <p className="text-lg font-medium text-gray-600">
                      {isAITalking ? 'AIãŒè©±ã—ã¦ã„ã¾ã™...' : recording ? 'ã‚ãªãŸã®ç•ªã§ã™' : 'å¾…æ©Ÿä¸­...'}
                    </p>
                    {recording && (
                      <button
                        onClick={stopRecording}
                        className="mt-4 px-6 py-2 bg-red-600 text-white rounded-full hover:bg-red-700 transition-colors"
                      >
                        è¿”ç­”ã‚’å®Œäº†
                      </button>
                    )}
                  </div>
                </>
              ) : (
                <div className="text-center bg-blue-50 p-8 rounded-lg">
                  <h3 className="text-xl font-medium text-blue-800 mb-4">é¢æ¥ã®æº–å‚™ãŒã§ãã¾ã—ãŸ</h3>
                  <p className="text-blue-700 mb-6">
                    AIã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¢ãƒ¼ãŒã‚ãªãŸã«è³ªå•ã‚’ã—ã¾ã™ã€‚
                    è³ªå•ã«å¯¾ã—ã¦è‡ªç„¶ã«è©±ã™ã ã‘ã§ã™ã€‚
                    é¢æ¥ã¯è‡ªå‹•çš„ã«é–‹å§‹ã•ã‚Œã¾ã™ã€‚
                  </p>
                  <div className="animate-bounce">
                    <svg className="mx-auto w-10 h-10 text-blue-500" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth="2" d="M19 13l-7 7-7-7m14-8l-7 7-7-7"></path>
                    </svg>
                  </div>
                </div>
              )}
            </div>
            
            {/* Previous conversation summary */}
            {conversationHistory.length > 2 && (
              <div className="p-4 border-t border-gray-200">
                <h3 className="text-sm font-medium text-gray-500 mb-2">ä¼šè©±ã®å±¥æ­´:</h3>
                <div className="max-h-48 overflow-y-auto">
                  {conversationHistory.slice(0, -2).map((message, index) => (
                    <div key={index} className="py-1 px-2 text-sm">
                      <span className={`font-medium ${message.role === 'assistant' ? 'text-indigo-600' : 'text-blue-600'}`}>
                        {message.role === 'assistant' ? 'AI: ' : 'ã‚ãªãŸ: '}
                      </span>
                      <span className="text-gray-700">
                        {message.content.length > 100 
                          ? message.content.substring(0, 100) + '...' 
                          : message.content}
                      </span>
                    </div>
                  ))}
                </div>
              </div>
            )}
          </div>
        </div>
      </main>

      {/* Feedback Modal */}
      {interviewEnded && feedbackText && (
        <Modal
          onClose={() => {}}
          closeOnOutsideClick={false}
          size="lg"
        >
          <div className="p-8">
            <h2 className="text-2xl font-bold mb-4">é¢æ¥å®Œäº†</h2>
            <div className="mb-6">
              <h3 className="text-lg font-medium mb-2">ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯</h3>
              <p className="text-gray-700">{feedbackText}</p>
            </div>
            <div className="flex justify-end">
              <button
                onClick={() => navigate('/dashboard')}
                className="px-4 py-2 bg-blue-600 text-white rounded-md hover:bg-blue-700"
              >
                ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«æˆ»ã‚‹
              </button>
            </div>
          </div>
        </Modal>
      )}
    </div>
  );
};